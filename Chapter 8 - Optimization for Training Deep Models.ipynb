{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep Learning Book (Simplified)\n",
    "## Part II - Modern Practical Deep Networks\n",
    "*This is a series of blog posts on the [Deep Learning book](http://deeplearningbook.org)\n",
    "where we are attempting to provide a summary of each chapter highlighting the concepts \n",
    "that we found to be most important so that other people can use it as a starting point\n",
    "for reading the chapters, while adding further explanations on few areas that we found difficult to grasp. Please refer [this](http://www.deeplearningbook.org/contents/notation.html) for more clarity on \n",
    "notation.*\n",
    "\n",
    "\n",
    "## Chapter 8: Optimization for Training Deep Models\n",
    "\n",
    "There are many types of optimization problems involved in deep learning, with the toughest one being that of training a neural network. The main theme of the chapter is to focus on one type of optimization - finding the parameters $\\theta$ that reduce a cost function $J(\\theta)$. <br>\n",
    "\n",
    "The chapter is organized as follows:\n",
    "\n",
    "**1. How Learning Differs from Pure Optimization** <br>\n",
    "**2. Challenges in Neural Network Optimization** <br>\n",
    "**3. Basic Algorithms** <br>\n",
    "**4. Parameter Initialization Strategies** <br>\n",
    "**5. Algorithms with Adaptive Learning Rates** <br>\n",
    "**6. Approximate Second-Order Methods** <br>\n",
    "**7. Optimization Strategies and Meta-Algorithms** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How Learning differs from Pure Optimization\n",
    "\n",
    "- In Machine Learning (ML), we care about a certain performance measure *P* (for e.g. accuracy) defined w.r.t the test set and optimize *$J(\\theta)$* (for e.g. cross-entropy loss) with the hope that it improves *P* as well. In pure optimization, optimizing *$J(\\theta)$* is the final goal.\n",
    "\n",
    "- The expected generalization error (**risk**) is taken over the true data-generating distribution $p_{data}$. If we do have that, it becomes an optimization problem. When we don't have $p_{data}$ but a finite training set, we have a ML problem. The latter can be converted back to an optimization problem by replace $p_{data}$ with the empirical distribution, $\\tilde{p}_{data}$ obtained from the training set, thereby reducing the empirical risk. This is called empirical risk minimization (ERM) and although it might look relatively similar to optimizationthere are two main problems:\n",
    "    - ERM is prone to overfitting with the possibility of the dataset being learned by high capacity models.\n",
    "    - ERM might not be feasible. Most optimization algorithms now are based on Gradient Descent (GD) which may not work with various loss functions like 0-1 loss (as it is not differentiable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, zero_one_loss\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.ones(201, dtype='float')\n",
    "y_pred = np.linspace(start=0, stop=1.0, num=201) # only the last prediction is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the losses for each example\n",
    "zero_one_losses = []\n",
    "cross_entropy_losses = []\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    zero_one_loss_value = zero_one_loss([y_true[i]], [y_pred[i] == y_true[i]])\n",
    "    cross_entropy_loss_value = log_loss([y_true[i]], [y_pred[i]], labels=[0, 1])\n",
    "    zero_one_losses.append(zero_one_loss_value)\n",
    "    cross_entropy_losses.append(cross_entropy_loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neat visualization hack\n",
    "y_max_allowed = 5\n",
    "for i, loss in enumerate(cross_entropy_losses):\n",
    "    if loss <= 5:\n",
    "        index_to_start = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efffd4eee50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_pred, zero_one_losses, label='zero_one_loss')\n",
    "plt.plot(y_pred[index_to_start:], cross_entropy_losses[index_to_start:], label='cross_entropy_loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Prediction value')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Comparison of zero-one loss and cross-entropy loss')\n",
    "plt.xticks(np.linspace(0, 1.4, 8))\n",
    "plt.yticks(np.linspace(0, np.max(cross_entropy_losses[index_to_start:]), 10))\n",
    "plt.tight_layout()\n",
    "plt.margins(0)\n",
    "plt.savefig('images/loss_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "- It is for the reasons mentioned above that a **surrogate loss function** (SLF) is used instead that acts as a proxy. E.g. the negative log-likelihood of the true class is used as a surrogate for 0-1 loss. Using a SLF might even turn out to be beneficial as you can keep continuing to obtain a better test error by pushing the classes even further apart to get a more reliable classifier.\n",
    "\n",
    "- Another common difference is that training might be halted following some convergence criterion based on Early Stopping to prevent overfitting, when the derivative of the surrogate loss function might still be large. This is different from pure optimization which is halted only when the derivative becomes very small. If you're not familiar with Early Stopping, I'd recommend you to look at our [previous post](https://medium.com/inveterate-learner/deep-learning-book-chapter-7-regularization-for-deep-learning-937ff261875c) where we talk about Early Stopping and other regularization techniques.\n",
    "\n",
    "- In ML optimization algorithms that objective function decomposes as a sum over the examples and we can perform updates by randomly sampling a batch of examples and taking the average over those examples. The Standard Error of the mean estimated from *n* examples is given by $\\frac {\\sigma} {\\sqrt{n}}$ indicating that as we include more examples for making an update, the returns of additonal examples in improving the error is less than linear. Thus, if we use 100 and 10000 example s to make an update, the latter takes 100 times more compute, but reducing the error only by a factor of 10. Thus, it's better to compute rapid approximate updates rather than a slow exact update.\n",
    "\n",
    "- There are 3 types of sampling based algorithms - **batch gradient descent (BGD)** where the entire training set is used to make a single update, **stochastic gradient descent (SGD)** where a single example is used to make an update and **mini-batch gradient descent (MBGD)** where a batch (not to be confused with BGD) of examples is randomly sampled from the entire training set and is used to make an update. *MBGD is nowadays commonly refered to as SGD*. It is a common practise to use batch sizes as powers of 2 to offer better runtime with certain hardware. Small batches tend to have a regularizing effect because of the noise they inject as each update is made by seeing only a very small portion of the entire training set, a.k.a., a batch of samples.\n",
    "\n",
    "- The minibatches should be selected randomly. It is sufficient to shuffle the dataset once and iterate over it multiple times. In the first epoch, the network sees each example for the first time and hence, the estimate of gradient is an *unbiased* estimate of the gradient of the true generalization error. However, from the second epoch onwards, the estimate becomes biased as it is resampling from data that it has already seen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Challenges in Neural Network Optimization\n",
    "\n",
    "The optimization problem for training neural networks is generally nonconvex. Some of the challenges faced are mentioned below:\n",
    "\n",
    "- **Ill-conditioning of the Hessian Matrix**: Can cause the SGD to get stuck in a sense that even very small steps increase the cost function. Moving by a factor of $-\\epsilon \\mathbf{g}$  would add the term given below to the cost function. Ill-conditioning is to happen when the first term exceeds the second term. In many cases, $\\mathbf{g}^TH\\mathbf{g}$ does increase significantly leading to slower learning due to reduced learning rate. Thus, while the gradient norm might be increasing, it can still lead to successful training.\n",
    "\n",
    "$$ \\frac{1}{2}\\epsilon^2\\mathbf{g}^TH\\mathbf{g} - \\epsilon\\mathbf{g}^T\\mathbf{g} $$\n",
    "\n",
    "- **Local minima**: Nearly any DL model is guaranteed to have an extremely large number of local minima (LM) arising due to the model identifiability problem.\n",
    "\n",
    "![minima](images/local_minima.gif)\n",
    "\n",
    "A model is said to be identifiable if a sufficiently large training set can rule out all but one setting of the model parameters. In case of neural networks, we can obtain equivalent models by swapping the position of the neurons. Thus, they are not identifiable.\n",
    "\n",
    "![model_identifiability](images/model_identifiability.png)\n",
    "\n",
    "Swapping the two hidden nodes leads to equivalent models. Thus, even after having a sufficiently large training set, there is not a unique setting of parameters. This is the model identifiability problem that neural networks suffer from.\n",
    "\n",
    "However, all the local minima caused due to this have the same value, thus not being a problem. However, if local minima with high cost are common, it becomes a serious problem. Many points other than local minima can lead to low gradients. Nowadays, it's common to aim for a low but not minimal cost value.\n",
    "\n",
    "\n",
    "- **Plateaus, Saddle Points and Other Flat Regions**: Saddle point (SP) is another type of point with zero gradient where some points around it have higher value and the others have lower. Intuitively, this means that a saddle point acts as both a local minima for some neighbors and a local maxima for the others. Thus, Hessian at SP has both +ve and -ve eigenvalues. \n",
    "\n",
    "![saddle](images/saddle_point.png)\n",
    "\n",
    "For many classes of random functions, saddle points become more common at high dimensions  with the ratio of number of SPs to LMs growing exponentially with *n* for an n-dimensional space. Many random functions have an amazing property that near points with lost cost, the Hessian tends to take up mostly positive eigenvalues. SGD empirically tends to rapidly avoid encountering a high-cost saddle point. There also might be wide, flat regions of constant value, thereby having a zero gradient. These can be problematic if the cost is high in these regions.\n",
    "\n",
    "![plateau](images/plateau.png)\n",
    "\n",
    "- **Cliffs and Exploding Gradients**: NNs might sometimes have extremely steep regions resembling cliffs due to the repeated multiplication of weights. At the edge of such a cliff, an update step might throw the parameters extremely far. It can be taken care of by using **gradient clipping (GC)**. The gradient indicates only the direction in which to make the update. If the GD update proposes making a very large step, GC intervenes to reduce the step size.\n",
    "![gradient clipping](images/gradient_clipping.png)\n",
    "\n",
    "- **Long-Term Dependencies**: This problem is encountered when the NN becomes sufficiently deep. For example, if the same weight matrix $\\mathbf{W}$ is used in each layer, after *t* steps, we'd get $\\mathbf{W}^t$. Using the eigendecomposition of $\\mathbf{W}$:\n",
    "\n",
    "$$ \\mathbf{W} = \\mathbf{V} diag(\\lambda) \\mathbf{V}^T $$\n",
    "$$ \\mathbf{W}^t = \\mathbf{V} diag(\\lambda)^t \\mathbf{V}^T $$\n",
    "\n",
    "Thus, any eigenvalues not near an absolute value of 1 would either explode or vanish leading to the [**vanishing and exploding gradient problem**](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/). The use of the same weight matrix is especially the case in RNNs where this is a serious problem.\n",
    "\n",
    "![vanish explode gradient](images/exploding_vanishing_gradient.jpg)\n",
    "\n",
    "- **Inexact Gradients**: Most optimization algorithms use a noisy/biased estimate of the gradient in cases where the estimate is based on sampling, while in cases where the true gradient is intractable for e.g. in the case of training a [Restricted Boltzmann Machine (RBM)](https://deeplearning4j.org/restrictedboltzmannmachine), an approximation of the gradient is used. For RBM, the [contrastive divergence algorithm](https://deeplearning4j.org/glossary.html#contrastivedivergence) gives a technique for approximating the gradient of its intractable log-likelihood\n",
    "\n",
    "\n",
    "- Neural Networks might not end up at any critical point at all and such critical points might not even necessarily exist. A lot of the problems might be avoided if there exists a space connected to reasonably directly to the solution by a path that local descent can follow and if we are able to initialize learning within that well-behaved space. Thus, choosing good initial points should be studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic Algorithms\n",
    "\n",
    "- **Stochastic Gradient Descent**: This has already been described before but there are certain things that should be kept in mind regarding SGD. The learning rate $\\epsilon$ is a very important parameter for SGD. $\\epsilon$ should be reduced after each epoch in general. This is due to the fact that the random sampling of batches acts as a source of noise which might make SGD keep oscillating around the minima without actually reaching it (the image [Source: https://goo.gl/tq6Xof] below shows this). The true gradient of the total cost function (involving the entire dataset) actually becomes 0 when we reach the minimum. Hence, BGD can use a fixed learning rate. <br>\n",
    "![decrease lr](images/decrease_lr.png)\n",
    "The following conditions guarantee convergence in case of SGD: \n",
    "![convergence of sgd](images/sgd_convergence.png) \n",
    "Practically, epsilon is decreased linearly until iteration $\\tau$: \n",
    "![lr update](images/lr_update.png)\n",
    "\n",
    "The main question is how to choose $\\epsilon_0$. Setting it too low makes the training proceed slowly which might lead to the algorithm being stuck at a high cost value. Setting it too high would lead to large oscillations which might even push the learning outside the optimal region. The best way is to monitor the first several iterations and set the learning rate to be higher than the best performing one, but not too high to cause instability. <br> <br>\n",
    "\n",
    "![lr_high_low](images/lr_high_low.png)\n",
    "\n",
    "A big advantage of SGD is that the time taken to compute a weight update doesn't grow with the number of training examples as each update is computed after observing a batch of samples which is independent of the total number of training examples. Theoretically, BGD makes the error rate $O(\\frac{1}{k})$ after *k* iterations whereas SGD makes it $O(\\frac{1}{\\sqrt{k}})$. However, SGD compensates for this with its advantages after a few iterations along with the ability to make rapid updates in the case of a large training set.\n",
    "\n",
    "- **Momentum**: The momentum algorithm accumulates the exponentially decaying moving average of past gradients (called as velocity) and uses it as the direction in which to take the next step. Momentum is given by mass times velocity, which is equal to velocity if we're using unit mass. The momentum update is given by:\n",
    "\n",
    "![momentum](images/momentum_update.png)\n",
    "\n",
    "The step size (earlier equal to learning rate * gradient) now depends on how large and aligned the sequence of gradients are. If the gradients at each iteration point in the same direction (say g), it will lead to a higher value of the step size as they just keep accumulating. Once it reaches a constant (terminal) velocity, the step size becomes ϵ || g|| / (1 - α). Thus, using α as 0.9 makes the speed 10 times. Common values of α are 0.5, 0.9 and 0.99.\n",
    "\n",
    "![momentum_visual](images/momentum.png)\n",
    "\n",
    "Viewing it as the Newtonian dynamics of a particle sliding down a hill, the momentum algorithm consists of solving a set of differential equations via numerical simulation. There are two kinds of forces involved as shown below:\n",
    "\n",
    "![momentum_newton](images/momentum_ball_roll.png)\n",
    "Momentum can be seen as two forces operating together. 1) Proportional to the negative of the gradient such that whenever it descends a steep part of the surface, it gathers speed and continues sliding in that direction until it goes uphill again. 2) A viscous drag force (friction) proportional to -v(t) without the presence of which the particle would keep oscillating back and forth as the negative of the gradient would keep forcing it to move downhill . Viscous force is suitable as it is weak enough to allow the gradient to cause motion and strong enough to resist any motion if the gradient doesn't justify moving.\n",
    "\n",
    "Read more about momentum in this excellent blog post by [distill.ai](http://www.distill.ai/): [Why Momentum Really Works](https://distill.pub/2017/momentum/).\n",
    "\n",
    "\n",
    "- **Nesterov Momentum**: This is a slight modification of the usual momentum equation. Here, the gradient is calculated after applying the current velocity to the parameters, which can be viewed as adding a correction factor.\n",
    "\n",
    "![nesterov](images/nesterov.png)\n",
    "\n",
    "The intuition behind Nesterov momentum is that upon being at a point θ in the parameter space, the momentum update is going to shift the point by $\\alpha$v. So, we are soon going to end up in the vicinity of ($\\theta$ + $\\alpha$v). Thus, it might be better to compute the gradient from that point onward. The figure below describes this visually:\n",
    "\n",
    "![nesterov_intuition](images/nesterov.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parameter Initialization Strategies\n",
    "\n",
    "Training algorithms for deep learning models are iterative in nature and require the specification of an initial point, which is extremely crucial as it decides whether the algorithm would converge or not and if it does, then is it as a point with a high cost or low cost. We have limited understanding of neural network optimization but the one property that we know with complete certainty is that the initialization should break symmetry. This means that if two hidden units are connected to the same input units, then these should have different initialization or else the gradient would update both the units in the same way and we don't learn anything new by using an additional unit. The idea of having each unit learn something different motivates random initialization of weights which is also computationally cheaper. Biases are often chosen heuristically and only the weights are randomly initialized, almost always from a Gaussian or uniform distribution. The scale of the distribution is of utmost concern. Large weights might have better symmetry-breaking effect but lead to chaos (extreme sensitivity to small perturbations in the input) and exploding values during forward & back prop. Iterative optimization algorithms like SGD tend to halt in areas near the initial values, thereby expressing a prior that the final parameters should be close to the initial values. Initializing $\\mathbf{\\theta}$ as $\\mathbf{\\theta_o}$ is similar to imposing a Gaussian prior $p(\\theta)$ with mean $\\theta_o$. Thus, is makes sense to choose $\\theta_o$ close to 0, which says that its less likely for the units to interact.\n",
    "\n",
    "Some heuristics include sampling from the following distributions:\n",
    "- $U(-\\frac{1}{\\sqrt{m}}, \\frac{1}{\\sqrt{m}})$\n",
    "- $U(-\\frac{6}{\\sqrt{n+m}}, \\frac{6}{\\sqrt{m+n}})$\n",
    "\n",
    "One drawback to using $\\frac{1}{\\sqrt{m}$ as the standard deviation is that the weights end up being  small when the layers are large. **Sparse initialization** sets each unit to have exactly *k* non-zero weights, motivated from the idea to have the total amount of input to each unit independent of the number of input units, *m*. However, it takes a long time for GD to correct incorrect large values and hence, this initialization might cause problems.  \n",
    "\n",
    "If the weights are too small, the range of activations across theminibatch will shrink as the activations propagate forward through the network.By repeatedly identifying the ﬁrst layer with unacceptably small activations andincreasing its weights, it is possible to eventually obtain a network with reasonableinitial activations throughout\n",
    " \n",
    "The biases are relatively easier to choose. They are mostly set to zero excluding a few situations (refer to book for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Algorithms with Adaptive Learning Rates\n",
    "\n",
    "- **AdaGrad**: It individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of historical squared values of the gradient. However, accumulation of squared gradients from the very beginning can lead to excessive and premature decrease in the learning rate.\n",
    "\n",
    "- **RMSProp**: It changes the gradient accumulation proposed in AdaGrad into an exponentially weighted moving average.  RMSProp uses the exponentially weighted average to discard history from the extreme past so that it can converge rapidly after finding a convex bowl, as if it were an instance of AdaGrad initialized within that bowl.\n",
    "\n",
    "- **Adam**: Adapted from \"adaptive moments\", it focuses on combining RMSProp and Momentum. Firstly, it views momentum as an estimate of the first-order moment and calculates the momentum and RMSProp terms. Secondly, it adds a correction term for both the moments to account for their initialization near the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Approximate Second-Order Methods\n",
    "\n",
    "We are using the empirical risk as the objective function.\n",
    "- **Newton's Method**: Uses the second-order Taylor Series expansion to approximate the $\\mathbf{\\it{J}(\\theta)}$ around a point $\\mathbf{\\theta_o}$ ignoring derivatives of higher order.<br>\n",
    "![newton](images/newton.png)\n",
    "We get the following critical point of the above equation:\n",
    "![newton point](images/newton_point.png)\n",
    "For quadratic surfaces, this directly gives the optimal result. However, for surfaces that are not quadratic, as long as the Hessian remains positive definite, we can obtain the optimal point through a 2-step iterative process - 1) Get the inverse of the Hessian and 2) update the parameters. <br><br>\n",
    "Saddle points are problematic for Newton's method. If all the eigenvalues are not positive, Newton's method might cause the updates to move in the wrong direction. A way to avoid this is to add regularization:\n",
    "![regularization](images/regularization.png)\n",
    "However, if there is a strong negative curvature, $\\alpha$ needs to be sufficiently large to offset the negative eigenvalues in which case the Hessian becomes dominated by the diagonal matrix thereby leading to an update which becomes the standard gradient divided by $\\alpha$. Another problem restricting the use of Newton's method is the computational cost. It takes $O(k^3)$ time to calculate the inverse of the Hessian where $k$ is the number of parameters. And since the parameters are updated every iteration, this inverse needs to be calculated at every iteration.\n",
    "\n",
    "\n",
    "- **Conjugate Gradients**: One weakness of the method of steepest descent is that line searches happen along the direction of the gradient. Suppose the previous search direction is $d_{t-1}$. Once the search terminates at the minimum, the next search direction, $d_t$ is given by the gradient at that point, which is perpendicular to $d_{t-1}$. Upon getting the minimum along the current search direction, the minimum along the previous search direction is not preserved, undoing, in a sense, the progress made in previous search direction. \n",
    "![conjugate gradient](images/conjugate_gradient.png)\n",
    "In the method of conjugate gradients, we seek a search direction that is conjugate to the previous line search direction:\n",
    "![conjugate_gd](images/conjugate_gd.png)\n",
    "with $d_t$ and $d_{t-1}$ being called as **conjugate** if $d^T_tHd_{t-1} = 0$.\n",
    "where $\\beta_t$ decides how much of $d_{t-1}$ is added back to the current search direction. There are two popular choices for $\\beta_t$ - Fletcher-Reeves and Polak-Ribière. These discussions assumed the cost function to be quadratic. To extend the concept to work for training neural networks, there is one additional change. Since it's no longer quadratic, there's no guarantee anymore than the conjugate direction would preserve the minimum in the previous search directions. Thus, the algorithm includes occasional resets where the method of conjugate gradients is restarted with line search along the unaltered gradient.\n",
    "\n",
    "- **BFGS**: This algorithm tries to bring the advantages of Newton's method without the additonal computational burden by approximating $H^{-1}$ by $M_t$, which is iteratively refined using low-rank updates. Finally, line search is conducted along the direction  $M_tg_t$. BFGS requires storing the matrix $M_t$ which takes $O(n^2)$ memory making it infeasible. An approach called Limited Memory BFGS (L-BFGS) has been proposed to tackle this infeasibility by computing the matrix $M_t$ using the same method as BFGS but assuming that $M_{t-1}$ is the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Optimization Strategies and Meta-Learning Algorithms\n",
    "\n",
    "- **Batch Normalization**: It is a method of adaptive reparameterization to tackle the difficulty of training deep models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
