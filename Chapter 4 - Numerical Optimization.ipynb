{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep Learning Book (Simplified)\n",
    "## Part I - Applied Math and Machine LearningÂ basics\n",
    "*This is a series of blog posts on the [Deep Learning book](http://deeplearningbook.org) where I am attempting to provide a summary of each chapter highlighting the concepts that I found to be most important so that other people can use it as a starting point for reading the chapters, while including the code for reproducing some of the results. Please refer [this](http://www.deeplearningbook.org/contents/notation.html) for more clarity on notation.*\n",
    "\n",
    "## Chapter 4: Numerical Computation\n",
    "\n",
    "Since you are here, there's a high probability that you must have heard of **Gradient Descent**. It is that part of a Deep Learning pipeline which leads to the model being *trained*. This chapter outlines the various kinds of numerical computations generally utilized by Machine Learning algorithms and also describes various optimization algorithms (e.g. Gradient Descent, Newton's method), which are those class of algorithms that update the estimates of the solution iteratively, rather than solving it analytically to provide a closed-form solution.\n",
    "\n",
    "The sections present in this chapter are listed below: <br>\n",
    "\n",
    "**1. Overflow and Underflow?** <br>\n",
    "**2. Poor Conditioning** <br>\n",
    "**3. Gradient-Based Optimization** <br>\n",
    "**4. Constrained Optimization** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overflow and Underflow\n",
    "\n",
    "There is a fundamental problem with representing infinitely many real numbers on a digital computer with a finite number of bit patterns, which is: it leads to rounding errors. Such rounding errors compound over certain operations and cause many theoretically correct algorithms to fail in practise. There are primarily two damaging forms of rounding errors:\n",
    "\n",
    "- **Underflow**: Underflow occurs when numbers near to zero are rounded down to zero. <br> \n",
    "The behaviour of certain functions like $\\frac{1}{x}$ , $log$, etc. can change dramatically due to this.\n",
    "\n",
    "- **Overflow**: Overflow occurs when a large number is approximated as $\\infty$ (or $-\\infty$).\n",
    "\n",
    "*Example* - Softmax\n",
    "![softmax](images/softmax.png)\n",
    "\n",
    "Assume every $x_i$ is equal to some $c$. <br> \n",
    "\n",
    "**Problems**:\n",
    "- $c$ is very negative: This leads to underflow when computing $exp(c)$ and thus $0$ in the denominator.\n",
    "- $c$ is very positive: This leads to overflow when computing $exp(c)$.\n",
    "\n",
    "**Solution**: \n",
    "\n",
    "Instead of computing $softmax(\\mathbf{x})$, we compute $softmax(\\mathbf{z})$, where $\\mathbf{z} = \\mathbf{x} - \\max_i x_i$. It can be proven that the value doesn't change after subtracting the same value from each of the elements. Now, the maximum value in $\\mathbf{z}$ is $0$, thus preventing overflow. Also, this ensures that atleast one element in the denominator is $1$, preventing underflow.\n",
    "\n",
    "*Food for thought*: This still doesn't prevent underflow in the numerator. Think of the case when the output from the softmax function is passed as input to another function, e.g., $log$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Poor Conditioning\n",
    "\n",
    "Conditioning measures how rapidly the output of a function changes with small changes in the input. Large conditioning means poor conditioning as rounding errors can lead to large changes in output.\n",
    "For e.g., let's observe: $ f(x) = A^{-1}x$. Given that $A \\in \\mathbb{R}^{n \\hspace{.1cm} \\text{x} \\hspace{.1cm} n}$ has an eigen value decomposition, its **condition number** is given by:\n",
    "\n",
    "![condition number](images/condition_number.png)\n",
    "\n",
    "which is equal to the ratio of the largest and the smallest eigen values. Having a large condition number signifies that matrix inversion is highly sensitive to errors in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
