{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep Learning Book (Simplified)\n",
    "## Part I - Applied Math and Machine Learning basics\n",
    "*This is a series of blog posts on the [Deep Learning book](http://deeplearningbook.org) where I am attempting to provide a summary of each chapter highlighting the concepts that I found to be most important so that other people can use it as a starting point for reading the chapters, while including the code for reproducing some of the results. Please refer [this](http://www.deeplearningbook.org/contents/notation.html) for more clarity on notation.*\n",
    "\n",
    "## Chapter 4: Numerical Computation\n",
    "\n",
    "Since you are here, there's a high probability that you must have heard of **Gradient Descent**. It is that part of a Deep Learning pipeline which leads to the model being *trained*. This chapter outlines the various kinds of numerical computations generally utilized by Machine Learning algorithms and also describes various optimization algorithms (e.g. Gradient Descent, Newton's method), which are those class of algorithms that update the estimates of the solution iteratively, rather than solving it analytically to provide a closed-form solution.\n",
    "\n",
    "The sections present in this chapter are listed below: <br>\n",
    "\n",
    "**1. Overflow and Underflow?** <br>\n",
    "**2. Poor Conditioning** <br>\n",
    "**3. Gradient-Based Optimization** <br>\n",
    "**4. Constrained Optimization** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overflow and Underflow\n",
    "\n",
    "There is a fundamental problem with representing infinitely many real numbers on a digital computer with a finite number of bit patterns, which is: it leads to rounding errors. Such rounding errors compound over certain operations and cause many theoretically correct algorithms to fail in practise. There are primarily two damaging forms of rounding errors:\n",
    "\n",
    "- **Underflow**: Underflow occurs when numbers near to zero are rounded down to zero. <br> \n",
    "The behaviour of certain functions like $\\frac{1}{x}$ , $log$, etc. can change dramatically due to this.\n",
    "\n",
    "- **Overflow**: Overflow occurs when a large number is approximated as $\\infty$ (or $-\\infty$).\n",
    "\n",
    "*Example* - Softmax\n",
    "![softmax](images/softmax.png)\n",
    "\n",
    "Assume every $x_i$ is equal to some $c$. <br> \n",
    "\n",
    "**Problems**:\n",
    "- $c$ is very negative: This leads to underflow when computing $exp(c)$ and thus $0$ in the denominator.\n",
    "- $c$ is very positive: This leads to overflow when computing $exp(c)$.\n",
    "\n",
    "**Solution**: \n",
    "\n",
    "Instead of computing $softmax(\\mathbf{x})$, we compute $softmax(\\mathbf{z})$, where $\\mathbf{z} = \\mathbf{x} - \\max_i x_i$. It can be proven that the value doesn't change after subtracting the same value from each of the elements. Now, the maximum value in $\\mathbf{z}$ is $0$, thus preventing overflow. Also, this ensures that atleast one element in the denominator is $1$, preventing underflow.\n",
    "\n",
    "*Food for thought*: This still doesn't prevent underflow in the numerator. Think of the case when the output from the softmax function is passed as input to another function, e.g., $log$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Poor Conditioning\n",
    "\n",
    "Conditioning measures how rapidly the output of a function changes with small changes in the input. Large conditioning means poor conditioning as rounding errors can lead to large changes in output.\n",
    "For e.g., let's observe: $ f(x) = A^{-1}x$. Given that $A \\in \\mathbb{R}^{n \\hspace{.1cm} \\text{x} \\hspace{.1cm} n}$ has an eigen value decomposition, its **condition number** is given by:\n",
    "\n",
    "![condition number](images/condition_number.png)\n",
    "\n",
    "which is equal to the ratio of the largest and the smallest eigen values. Having a large condition number signifies that matrix inversion is highly sensitive to errors in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Constrained Optimization\n",
    "\n",
    "It might be the case that although we want to maximize (or minimize) $f(x)$, but aren't allowed to use all possible values of $x$, say $x \\in \\mathbb{S}$, for some set $\\mathbb{S}$. This now becomes a problem of **Constrained Optimization**. The points $\\mathbf{x}$ in $S$ are called **feasible points**. \n",
    "\n",
    "An example of such a constraint can be the L2-norm constraint, e.g. $|| \\hspace{.1cm} x \\hspace{.1cm}||^2 < 1$. This is useful as we often want the values for our weights to be small (i.e. close to $0$).\n",
    "\n",
    "*Approach*: Design a separate, unconstrained optimization problem, whose solution can be converted to the original constrained optimization problem. E.g. in the above described constrained optimization problem, we could instead minimize:\n",
    "$$g(\\theta) = f([\\cos\\theta, \\sin\\theta]^T)$$\n",
    "\n",
    "with respect to $\\theta$ and return ($\\cos\\theta, \\sin\\theta$).\n",
    "\n",
    "\n",
    "General solution: **Karush–Kuhn–Tucker(KKT)** approach which introduces a **generalized Lagrangian**.\n",
    "\n",
    "Approach: \n",
    "\n",
    "We use $m$ functions $g^{(i)}(x)$ and $n$ functions $h^{(j)}(x)$ to describe $\\mathbb{S}$,  such that any element $x \\in \\mathbb{S}$ satisfies: \n",
    "$$g^{(i)}(x) = 0 \\hspace{.1cm} \\text{and} \\hspace{.1cm} h^{(j)}(x) \\leq 0 \\hspace{.1cm} \\forall \\hspace{.1cm} i, j$$\n",
    "\n",
    "There are two constraints specified here. I'll explain them with an example. Let's take $g(x)$ as $x - 2$ and $h(x)$ as $x-3$. <br>\n",
    "Then for $x = 2$, we have the following:\n",
    "\n",
    "- **Equality constraints**: $g^{(i)}(x) = 0$. Here, $g(2) = 0$. Hence, $x = 2$ satisfies the equality constraints.\n",
    "- **Inequality constraints**: $h^{(i)}(x) \\leq 0$. Here, $h(2) = -1 < 0$. Hence, $x = 2$ satisfies the inequality constraints.\n",
    "\n",
    "Note that for $x = 3$, $h(x)$ is an equality constraint that it satisfies whereas $g(x)$ is neither.\n",
    "\n",
    "New paramaters (called KKT multipliers): $\\lambda_i$, $\\alpha_j$ for each constraint.  <br>\n",
    "Generalized Lagrangian:\n",
    "\n",
    "\n",
    "![lagrangian](images/Lagrangian.png)\n",
    "\n",
    "Now, let: $Y =\\max\\limits_{\\alpha} \\max\\limits_{\\lambda} L(x, \\lambda, \\alpha)$\n",
    "Then, $\\min\\limits_x(f(x)) = \\min\\limits_x(Y)$\n",
    "\n",
    "This is because, if the constraints are satisfied, $Y = f(x)$ and if it isn't, $Y = \\infty$. This ensures that only feasible points are optimal. For finding the maximum of f(x), we can use the same generalized Lagrangian applied on $-f(x)$. \n",
    "\n",
    "The inequality constraints need to be observed more closely. Suppose the optimal point comes out to be $x^*$. If $h^{(i)}(x^*) = 0$, then the constraint is said to be **active**. However, if the constraint is inactive, i.e. $h^{(i)}(x^*) < 0$, then even if we remove the constraint, $x^*$ continues to be a local solution. Also, by definition, an inactive $h^{(i)}$ is negative and hence $\\max\\limits_{\\alpha} \\max\\limits_{\\lambda} L(x, \\lambda, \\alpha) \\Rightarrow \\alpha_i = 0$. Thus, either $\\alpha_i = 0$ or $h^{(i)}(x^*) = 0$ (in the case of active constraint). Hence, $\\mathbf{\\alpha} \\odot h{(x)} = 0$.\n",
    "\n",
    "Intuition: \n",
    "\n",
    "The relation of the optimal point can satisfy only of these two conditions:\n",
    "\n",
    "- The point is at the boundary of the constraint (i.e. active), then the corresponding KKT multiplier should be used.\n",
    "\n",
    "- The constraint has no influence in the evaluation of the point and hence, the corresponding KKT multiplier is zeroed out.\n",
    "\n",
    "The optimal points satisfy the following KKT conditions, which are necessary but not always sufficient:\n",
    "\n",
    "![kkt](images/kkt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
