{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep Learning Book (Simplified)\n",
    "## Part II - Modern Practical Deep Networks\n",
    "*This is a series of blog posts on the [Deep Learning book](http://deeplearningbook.org)\n",
    "where we are attempting to provide a summary of each chapter highlighting the concepts \n",
    "that we found to be most important so that other people can use it as a starting point\n",
    "for reading the chapters, while including the code for reproducing some of the results. \n",
    "Please refer [this](http://www.deeplearningbook.org/contents/notation.html) for more clarity on \n",
    "notation.*\n",
    "\n",
    "\n",
    "## Chapter 7: Regularization for Deep Learning\n",
    "\n",
    "Recalling from Chapter 5, **overfitting** is said to occur when the training error keeps decreasing but the test error (or the generalization error) starts increasing. **Regularization** is the modification we make to a learning algorithm that reduces its generalization error, but not its training error. There are various ways of doing this, some of which include restriction on parameter values or adding terms to the objective function, etc.\n",
    "\n",
    "These constraints are designed to encode some sort of prior knowledge, with a preference towards simpler models to promote generalization (See [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor)). The sections present in this chapter are listed below: <br>\n",
    "\n",
    "**1. Parameter Norm Penalties** <br>\n",
    "**2. Norm Penalties as Constrained Optimization** <br>\n",
    "**3. Regularization and Under-Constrained Problems** <br>\n",
    "**4. Dataset Augmentation** <br>\n",
    "**5. Noise Robustness** <br>\n",
    "**6. Semi-Supervised Learning** <br>\n",
    "**7. Mutlitask Learning** <br>\n",
    "**8. Early Stopping** <br>\n",
    "**9. Parameter Tying and Parameter Sharing** <br>\n",
    "**10. Sparse Representations** <br>\n",
    "**11. Bagging and Other Ensemble Methods** <br>\n",
    "**12. Dropout** <br>\n",
    "**13. Adversarial Training** <br>\n",
    "**14. Tangent Distance, Tangent Prop and Manifold Tangent Classifier** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter Norm Penalties\n",
    "\n",
    "The idea here is to limit the capacity (the space of all possible model families) of the model \n",
    "by adding a parameter norm <br>\n",
    "penalty, $\\Omega(\\theta)$, to the objective function, $J$:\n",
    "\n",
    "$$ \\tilde{J}(\\theta; X, y) =  J(\\theta; X, y) + \\lambda \\Omega(\\theta)$$\n",
    "\n",
    "Here, $\\theta$ represents only the weights and not the biases, the reason being that the biases require much less data to fit and do not add much variance.\n",
    "\n",
    "**1.1 $L^2$ Parameter Regularization**\n",
    "\n",
    "Here, the parameter norm penalty:\n",
    "$$\\Omega(\\theta) = \\frac {||w||_2^2} {2}$$\n",
    "\n",
    "This makes the objective function:\n",
    "\n",
    "$$ \\tilde{J} (\\theta; X, y) = J(\\theta; X, y) + \\alpha \\frac {w^T w} {2} $$\n",
    "\n",
    "Applying the 2nd order Taylor-Series approximation at the point $w^*$ where $\\tilde{J} (\\theta; X, y)$ assumes the minimum value, i.e., $\\bigtriangledown_w \\tilde {J} (w^*) = 0$:\n",
    "\n",
    "$$ \\hat{J}(w) = J(w^*) + \\frac{(w - w^*)^T H(J(w^*))(w - w^*)} {2} $$\n",
    "\n",
    "Finally, $\\bigtriangledown_w \\hat{J}(w) = H(J(w^*))(w - w^*)$ and the overall gradient of the objective function becomes:\n",
    "\n",
    "$$ \\bigtriangledown_w \\tilde{J}(w) = H(J(w^*))(\\tilde{w} - w^*) + \\alpha \\tilde{w} = 0$$\n",
    "$$ \\tilde{w} = (H + \\alpha I)^{-1} H w^* $$\n",
    "\n",
    "As $\\alpha$ approaches 0, $w$ comes closer to $w^*$. Finally, since $H$ is real and symmetric, it can be decomposed into a diagonal matrix $\\wedge$ and an orthonormal set of eigenvectors, $Q$. That is, $H = Q^T\\wedge Q$.\n",
    "\n",
    "![l2 reg](images/L2_reg.png)\n",
    "\n",
    "Because of the marked term, the value of each weight is rescaled along the eigenvectors of $H$. The value of the weights along the $i^{th}$ eigenvector is rescaled by $\\frac {\\lambda_i}{\\lambda_i + \\alpha}$, where $\\lambda_i$ represents the eigenvalue corresponding to the $i^{th}$ eigenvector.\n",
    "\n",
    "| Condition| Effect |\n",
    "| --- | --- |\n",
    "|  $\\lambda_i >> \\alpha$ | not much effect |\n",
    "|  $\\lambda_i << \\alpha$ | The weight value almost shrunk to zero |\n",
    "\n",
    "The diagram below illustrates this well.\n",
    "\n",
    "![L2 scaling](images/L2_scaling.png)\n",
    "\n",
    "To look at its application to Machine Learning, we have to look at linear regression. The objective function there is exactly quadratic, given by:\n",
    "\n",
    "![linear_reg](images/linear_reg.png)\n",
    "\n",
    "**1.2 $L^1$ parameter regularization**\n",
    "\n",
    "Here, the parameter norm penalty:\n",
    "$$\\Omega(\\theta) = ||w||_1 $$\n",
    "\n",
    "Making the gradient of the overall objective function:\n",
    "\n",
    "$$ \\bigtriangledown_w \\tilde{J}(\\theta; X, y) = \\bigtriangledown_w J(\\theta; X, y) + \\alpha * sign(w) $$\n",
    "\n",
    "Now, the last term, sign(w), create a difficulty that the gradient no longer scales linearly with $w$. This leads to a few complexities in arriving at the optimal solution (which I am going to skip):\n",
    "![l1_reg](images/l1_reg.png)\n",
    "\n",
    "Our current interpretation of the `max` term is that, there shouldn't be a zero crossing, as the gradient of the absolute value function is not differentiable at zero.\n",
    "\n",
    "![lasso result](images/lasso_result.png)\n",
    "\n",
    "\n",
    "Thus, $L^1$ regularization has the property of sparsity, which is its fundamental distinguishing feature from $L^2$. Hence, $L^1$ is used for feature selection as *LASSO*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Norm penalties as constrained optimization\n",
    "\n",
    "From chapter 4's section 4, we know that to minimize any function under some constraints, we can construct a generalized Lagrangian function containing the objective function along with the penalties. Suppose we wanted $\\Omega(\\theta)) < k$, then we could construct the following Lagrangian:\n",
    "![lagrangian](images/lagrangian.png)\n",
    "\n",
    "Thus, $\\theta^* = argmin_{\\theta} (max_{\\alpha, \\alpha >= 0} \\hspace{.2cm} \\mathcal{L}(\\theta, \\alpha; X, y))$. If  $\\Omega(\\theta) > k$, then $\\alpha$ should be large to reduce its value below k. <br>\n",
    "Likewise, if $\\Omega(\\theta) < k$, then $\\alpha$ should be small. Assuming $\\alpha$ to be a constant $\\alpha^{*}$:\n",
    "\n",
    "$$ \\theta^* = argmin_{\\theta} \\hspace{.2cm} J(\\theta; X, y) + \\alpha^* \\Omega(\\theta)$$\n",
    "\n",
    "This is now similar to the parameter norm penalty regularized objective function. Thus, parameter norm penalties naturally impose a constraint, like the L2-regularization defining a constrained L2-ball. Larger $\\alpha$ means a smaller constrained region and vice versa. The idea of constraints over penalties, is important for several reasons. Penalties might cause non-convex optimization algorithms to get stuck in local minima due to small values of $\\theta$, leading to the formation of so-called `dead cells`, as the weights entering and leaving them are too small to have an impact. Constraints don't enforce the weights to be near zero, rather being confined to a constrained region.\n",
    "\n",
    "Another reason is that constraints induce higher stability. With higher learning rates, there might be a large weight, leading to a large gradient, which could go on iteratively leading to numerical overflow in the value of $\\theta$. Constrains along with reprojection (to the corresponding ball) prevent the weights from becoming too large, thus, maintaining stability. \n",
    "\n",
    "A final suggestion made by Hinton was to restrict the individual column norms of the weight matrix rather than the Frobenius norm of the entire weight matrix, so as to prevent any hidden unit from having a large weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regularized & Under-constrained problems\n",
    "\n",
    "Underdetermined problems are those problems that have infinitely many solutions. In some machine learning problems, regularization is necessary. For e.g., many algorithms (e.g. PCA) require the inversion of $X^TX$, which might be singular. In such a case, we can use a regularized form instead. $(X^TX + \\alpha I)$ is guaranteed to be invertible. A logistic regression problem having linearly separable classes with $w$ as a solution, will always have $2w$ as a solution and so on.\n",
    "\n",
    "Finally, regularization can solve underdetermined problems. For e.g. the Moore-Pentose pseudoinverse  defined earlier as:\n",
    "\n",
    "$$ X^+ = lim_{\\alpha \\rightarrow 0} (X^TX + \\alpha I)^{-1}X^T $$\n",
    "\n",
    "This can be seen as performing a linear regression with L2-regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data augmentation\n",
    "\n",
    "Having more data is the most desirable thing to improving a machine learning model's performance. In many cases, it is relatively easy to artifically generate data. For a classification task, we desire for the model to be invariant to certain types of transformations, and we can generate the corresponding $(x, y)$ pairs by translating the input $x$. But for certain problems, like density estimation, we can't apply this directly unless we have already solved the density estimation problem. \n",
    "\n",
    "However, caution needs to be mentioned while data augmentation to make sure that the class doesn't change.  For e.g., if the labels contain both \"b\"  and \"d\", then horizontal flipping would be a bad idea for data augmentation. Add random noise to the inputs is another form of data augmentation, while adding noise to hidden units can be seen as doing data augmentation at multiple levels of abstraction.\n",
    "\n",
    "Finally, when comparing machine learning models, we need to compare them on the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
