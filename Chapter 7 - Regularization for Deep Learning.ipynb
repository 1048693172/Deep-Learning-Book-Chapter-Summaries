{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep Learning Book (Simplified)\n",
    "## Part II - Modern Practical Deep Networks\n",
    "*This is a series of blog posts on the [Deep Learning book](http://deeplearningbook.org)\n",
    "where we are attempting to provide a summary of each chapter highlighting the concepts \n",
    "that we found to be most important so that other people can use it as a starting point\n",
    "for reading the chapters, while including the code for reproducing some of the results. \n",
    "Please refer [this](http://www.deeplearningbook.org/contents/notation.html) for more clarity on \n",
    "notation.*\n",
    "\n",
    "\n",
    "## Chapter 7: Regularization for Deep Learning\n",
    "\n",
    "Recalling from Chapter 5, **overfitting** is said to occur when the training error keeps decreasing but the test error (or the generalization error) starts increasing. **Regularization** is the modification we make to a learning algorithm that reduces its generalization error, but not its training error. There are various ways of doing this, some of which include restriction on parameter values or adding terms to the objective function, etc.\n",
    "\n",
    "These constraints are designed to encode some sort of prior knowledge, with a preference towards simpler models to promote generalization (See [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor)). The sections present in this chapter are listed below: <br>\n",
    "\n",
    "**1. Parameter Norm Penalties** <br>\n",
    "**2. Norm Penalties as Constrained Optimization** <br>\n",
    "**3. Regularization and Under-Constrained Problems** <br>\n",
    "**4. Dataset Augmentation** <br>\n",
    "**5. Noise Robustness** <br>\n",
    "**6. Semi-Supervised Learning** <br>\n",
    "**7. Mutlitask Learning** <br>\n",
    "**8. Early Stopping** <br>\n",
    "**9. Parameter Tying and Parameter Sharing** <br>\n",
    "**10. Sparse Representations** <br>\n",
    "**11. Bagging and Other Ensemble Methods** <br>\n",
    "**12. Dropout** <br>\n",
    "**13. Adversarial Training** <br>\n",
    "**14. Tangent Distance, Tangent Prop and Manifold Tangent Classifier** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter Norm Penalties\n",
    "\n",
    "The idea here is to limit the capacity (the space of all possible model families) of the model \n",
    "by adding a parameter norm <br>\n",
    "penalty, $\\Omega(\\theta)$, to the objective function, $J$:\n",
    "\n",
    "$$ \\tilde{J}(\\theta; X, y) =  J(\\theta; X, y) + \\lambda \\Omega(\\theta)$$\n",
    "\n",
    "Here, $\\theta$ represents only the weights and not the biases, the reason being that the biases require much less data to fit and do not add much variance.\n",
    "\n",
    "**1.1 $L^2$ Parameter Regularization**\n",
    "\n",
    "Here, the parameter norm penalty:\n",
    "$$\\Omega(\\theta) = \\frac {||w||_2^2} {2}$$\n",
    "\n",
    "This makes the objective function:\n",
    "\n",
    "$$ \\tilde{J} (\\theta; X, y) = J(\\theta; X, y) + \\alpha \\frac {w^T w} {2} $$\n",
    "\n",
    "Applying the 2nd order Taylor-Series approximation at the point $w^*$ where $\\tilde{J} (\\theta; X, y)$ assumes the minimum value, i.e., $\\bigtriangledown_w \\tilde {J} (w^*) = 0$:\n",
    "\n",
    "$$ \\hat{J}(w) = J(w^*) + \\frac{(w - w^*)^T H(J(w^*))(w - w^*)} {2} $$\n",
    "\n",
    "Finally, $\\bigtriangledown_w \\hat{J}(w) = H(J(w^*))(w - w^*)$ and the overall gradient of the objective function becomes:\n",
    "\n",
    "$$ \\bigtriangledown_w \\tilde{J}(w) = H(J(w^*))(\\tilde{w} - w^*) + \\alpha \\tilde{w} = 0$$\n",
    "$$ \\tilde{w} = (H + \\alpha I)^{-1} H w^* $$\n",
    "\n",
    "As $\\alpha$ approaches 0, $w$ comes closer to $w^*$. Finally, since $H$ is real and symmetric, it can be decomposed into a diagonal matrix $\\wedge$ and an orthonormal set of eigenvectors, $Q$. That is, $H = Q^T\\wedge Q$.\n",
    "\n",
    "![l2 reg](images/L2_reg.png)\n",
    "\n",
    "Because of the marked term, the value of each weight is rescaled along the eigenvectors of $H$. The value of the weights along the $i^{th}$ eigenvector is rescaled by $\\frac {\\lambda_i}{\\lambda_i + \\alpha}$, where $\\lambda_i$ represents the eigenvalue corresponding to the $i^{th}$ eigenvector.\n",
    "\n",
    "| Condition| Effect |\n",
    "| --- | --- |\n",
    "|  $\\lambda_i >> \\alpha$ | not much effect |\n",
    "|  $\\lambda_i << \\alpha$ | The weight value almost shrunk to zero |\n",
    "\n",
    "The diagram below illustrates this well.\n",
    "\n",
    "![L2 scaling](images/L2_scaling.png)\n",
    "\n",
    "To look at its application to Machine Learning, we have to look at linear regression. The objective function there is exactly quadratic, given by:\n",
    "\n",
    "![linear_reg](images/linear_reg.png)\n",
    "\n",
    "**1.2 $L^1$ parameter regularization**\n",
    "\n",
    "Here, the parameter norm penalty:\n",
    "$$\\Omega(\\theta) = ||w||_1 $$\n",
    "\n",
    "Making the gradient of the overall objective function:\n",
    "\n",
    "$$ \\bigtriangledown_w \\tilde{J}(\\theta; X, y) = \\bigtriangledown_w J(\\theta; X, y) + \\alpha * sign(w) $$\n",
    "\n",
    "Now, the last term, sign(w), create a difficulty that the gradient no longer scales linearly with $w$. This leads to a few complexities in arriving at the optimal solution (which I am going to skip):\n",
    "![l1_reg](images/l1_reg.png)\n",
    "\n",
    "Our current interpretation of the `max` term is that, there shouldn't be a zero crossing, as the gradient of the absolute value function is not differentiable at zero.\n",
    "\n",
    "![lasso result](images/lasso_result.png)\n",
    "\n",
    "\n",
    "Thus, $L^1$ regularization has the property of sparsity, which is its fundamental distinguishing feature from $L^2$. Hence, $L^1$ is used for feature selection as *LASSO*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
