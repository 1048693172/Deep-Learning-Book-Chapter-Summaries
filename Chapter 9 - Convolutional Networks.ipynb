{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep Learning Book (Simplified)\n",
    "## Part II - Modern Practical Deep Networks\n",
    "*This is a series of blog posts on the [Deep Learning book](http://deeplearningbook.org)\n",
    "where we are attempting to provide a summary of each chapter highlighting the concepts \n",
    "that we found to be most important so that other people can use it as a starting point\n",
    "for reading the chapters, while adding further explanations on few areas that we found difficult to grasp. Please refer [this](http://www.deeplearningbook.org/contents/notation.html) for more clarity on \n",
    "notation.*\n",
    "\n",
    "\n",
    "## Chapter 9: Convolutional Networks\n",
    "\n",
    "**Convolutional networks**, also known as **convolutional neural networks**, or **CNN**s, are a specialized kind of neural network for processing data that has a known grid-like topology. <br>\n",
    "\n",
    "The chapter is organized as follows:\n",
    "\n",
    "**1. The Convolution Operation** <br>\n",
    "**2. Motivation** <br>\n",
    "**3. Pooling** <br>\n",
    "**4. Convolution and Pooling as an Infinitely Strong Prior** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Convolution Operation\n",
    "\n",
    "- The convolution operates on the **input** with a **kernel** (weights) to produce an **output map** given by:\n",
    "$$ s(t) = \\int x(a)w(t-a)da $$\n",
    "or in discrete space as:\n",
    "$$ s(t) = \\sum_{a=-\\infty}^{\\infty} x(a)w(t-a) $$\n",
    "and in 2D as:\n",
    "$$ s(i,j) = \\sum_m \\sum_n x(i,j)w(i-m,j-n) $$\n",
    "- The flipping of the kernel weights gives the formulation the commutative property, i.e.\n",
    "$$ s(i,j) = \\sum_m \\sum_n x(i,j)w(i-m,j-n) = \\sum_m \\sum_n x(i-m,j-n)w(i,j) $$\n",
    "- When the kernel isn't flipped it results in the **cross-correlation**:\n",
    "$$ s(i,j) = \\sum_m \\sum_n x(i+m,j+n)w(i,j) $$\n",
    "This operation however lacks the commutative property\n",
    "- The operation can be broken into matrix multiplications using the **Toeplitz** matrix representation for 1D and **block-circulant** matrix for 2D convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Motivation\n",
    "\n",
    "- **Sparse interactions**: Each output unit is connected to (affected by) only a subset of the input units. This significantly reduces the number of parameters and hence the number of computations in the operation. In  *deep CNNs*, the units in the deeper layers interact *indirectly* with large subsets of the input which allows modelling of complex interactions through sparse connections.\n",
    "\n",
    "- **Parameter sharing**: In a CNN each kernel weight is used at every input position (except maybe at boundaries where different padding rules apply). It can be seen easily that if the same linear operation needs to be applied at all positions in an input image, the convoution representations is much more economical as compared to the equivalent fully-connected variant. Less parameters also implies more statistical efficiency.\n",
    "\n",
    "- **Equivariance**: Parameter sharing also provides **equivariance to translation**\n",
    "    - A function *f* is said to be equivarient to a function *g* if $f(g(x)) = g(f(x))$ i.e. if input changes, the output changes in the same way\n",
    "    - Here we see the translation of the image results in corresponding translation in the output map (except maybe for boundary pixels)\n",
    "    - Note that convolution operation by itself is not equivariant to changes in scale or rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pooling\n",
    "\n",
    "A convolutional layer can be broken into the following components:\n",
    "\n",
    "1. Convolution\n",
    "2. Activation (detector stage)\n",
    "3. Pooling\n",
    "\n",
    "\n",
    "- The pooing function calculates a **summary statistic** of the nearby pixels at the point of operation. Several common statistics are max, mean, weighted average and $L^2$ norm of a surrounding rectangular window.\n",
    "- Pooling makes the representation slightly **translation invariant** in that small translations in the input do not cause large changes in the output map. It allows detection of a particular feature if we only care about its existence, not its position in an image. This is a strong requirement on the representation learnt.\n",
    "- Pooling over feature channels can be used to develop invariance to certain transformations of the input. For e.g., units in a layer may be developed to learn rotated features and then pooled over. This property has been used in [maxout networks](http://proceedings.mlr.press/v28/goodfellow13.pdf)\n",
    "- Pooling reduces the input size to the next layer in turn reducing the number of computations required upstream.\n",
    "- Variable sized inputs are an issue when presented to a fully connected layer. To counter this, the pooling operation maybe performed on regions of the input (such as quadrants) thus allowing the model to work on variable sized inputs.\n",
    "\n",
    "\n",
    "[Theoretical guidelines](http://www.di.ens.fr/willow/pdfs/icml2010b.pdf) for which pooling to use have been studied. [Dynamic pooling](http://yann.lecun.com/exdb/publis/pdf/boureau-iccv-11.pdf) has also been studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convolution and Pooling as an Infinitely Strong Prior\n",
    "\n",
    "**What is a weight prior?** Assumptions about the weights (before learning) in terms of acceptable values and range are encoded into the *prior* distribution of the weights. A *weak prior* is has a high variance and shows that there is low confidence in the initial value of the weight. A *strong prior* is turn shows a narrow range of values about which we are confident before learning begins. An *infinitely strong prior* demarkates certain values as forbidden completely assigning them zero probability.\n",
    "\n",
    "If we view the convolutional layer as a fully connected layer, **convolution imposes an infinitely strong prior** by making the following restrictions on the weights:\n",
    "1. Adjacent units must have the same weight but shifted in space\n",
    "2. Except for a small spatially connected region, all other weights must be zero\n",
    "\n",
    "\n",
    "Likewise the **pooling stage imposes an infinitely strong prior** by requiring features to be translation invariant.\n",
    "\n",
    "Insights:\n",
    "1. Conv and pooling can cause underfitting if the priors imposed are not suitable for the task.\n",
    "2. Convolutional models should only be compared with other convolutional models. This is because other models which are **pertumation invariant** can learn even when input features are permuted (thus loosing spatial relationships). Such models need to learn these spatial relationships (which are hard coded in CNNs). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
